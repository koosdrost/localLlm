spring.application.name=demo-AI
server.port=8080

# === OLLAMA CONNECTIE ===
## Waar draait je Ollama server?
#spring.ai.ollama.base-url=http://localhost:11434
#
## === MODEL SELECTIE === ?? BELANGRIJK
## Welk model gebruik je? Moet tool calling ondersteunen!
## Werkende modellen: qwen2.5, llama3.1, mistral
#spring.ai.ollama.chat.options.model=qwen2.5:7b
#
## === GENERATION PARAMETERS ===
## Temperature: 0.0 = deterministisch, 1.0 = creatief
#spring.ai.ollama.chat.options.temperature=0.7
#
## Max tokens in response (hoger = langere antwoorden mogelijk)
#spring.ai.ollama.chat.options.num-predict=2000
#
## Top-p (nucleus sampling): 0.9 is goed voor meeste taken
#spring.ai.ollama.chat.options.top-p=0.9
#
## Top-k: Beperkt woordkeuzes (40 is default, werkt prima)
#spring.ai.ollama.chat.options.top-k=40
#
## === ADVANCED (meestal niet nodig) ===
## Repeat penalty: voorkomt herhaling (1.1 is default)
#spring.ai.ollama.chat.options.repeat-penalty=1.1

# Context window: hoeveel tokens history (default is model-afhankelijk)
# spring.ai.ollama.chat.options.num-ctx=4096

# === LOGGING (handig voor debugging) ===
logging.level.org.springframework.ai=INFO
logging.level.com.example.agent=DEBUG

# === DATABASE (als je die gebruikt) ===
#spring.datasource.url=jdbc:postgresql://localhost:5432/mydb
#spring.datasource.username=user
#spring.datasource.password=pass
#spring.jpa.hibernate.ddl-auto=update